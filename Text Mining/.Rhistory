plot(fit_Avg)
rect.hclust(fit_Avg,k=7,border="red")
# Clusters using Complete linkage method
fit_Compl <- hclust(d,method="complete")
windows()
plot(fit_Compl)
rect.hclust(fit_Compl,k=6,border="red")
rect.hclust(fit_Compl,k=4,border="red")
# Clusters using single linkage method
fit_Sngl <- hclust(d,method="single")
# Clusters using single linkage method
fit_Sngl <- hclust(d,method="single")
plot(fit_Sngl)
plot(fit_Sngl)
#############Hierarchical clustering ###################
install.packages("dendextend")
library(dendextend)
cd<-colour_branches(fit_Avg,k=7)
# Clusters using average linkage method
fit_Avg <- hclust(d,method="average")
windows()
plot(fit_Avg)
cd<-colour_branches(fit_Avg,k=7)
library(plyr)
km4 <- kmeans(Airlines_Data_Forclustering,5)
windows()
#Scree plot
wss <- c()
for (i in 2:15)
wss[i] <- sum(kmeans(Airlines_Data_Forclustering,centers=i)$withinss)
plot(1:15,wss,type="b",xlab="K",ylab="Avg Distance-within cluster")
# based on scree plot, let's try 9 clusters
km9 <- kmeans(Airlines_Data_Forclustering,9)
Clusters_KM9 <- data.frame(Airlines_Data_Forclustering,km9$cluster)
Clusters_KM9
cl<- kmeans.ani(crime_data_scaled, 9)
library(animation)
cl<- kmeans.ani(crime_data_scaled, 9)
cl<- kmeans.ani(Airlines_Data_Forclustering, 9)
write.csv(Clusters_KM9,file="E:/Assignments/Clustering/Airline.csv")
write.csv(Clusters_KM9,file="E:/Assignments/Clustering/Book2.csv")
write.csv(Clusters_KM9,file="filnal.csv")
library(partykit)
library(C50)
library(tree)
library(gmodels)
library(caret)
CompanyData <- read.csv(file.choose())
hist(CompanyData$Sales)
range(CompanyData$Sales)
High<-ifelse(CompanyData$Sales<9, "No", "Yes")
CD<-data.frame(CompanyData, High)
CD_train <- CD[1:200,]
CD_test <- CD[201:400,]
#Using Party Function
op_tree = ctree(High ~ CompPrice + Income + Advertising + Population + Price + ShelveLoc
+ Age + Education + Urban + US, data = CD_train)
summary(op_tree)
plot(op_tree)
pred_test <- predict(op_tree,newdata=CD_test)
pred_test
CrossTable(CD_test$High,pred_test)
confusionMatrix(CD_test$High,pred_test) # Accuracy = 76%
##### Using tree function
cd_tree <- tree(High~.-Sales,data=CD_train)
summary(cd_tree)
plot(cd_tree)
text(cd_tree,pretty = 0)
pred_tree <- as.data.frame(predict(cd_tree,newdata=CD_test))
pred_tree["final"] <- NULL
pred_test_1 <- predict(cd_tree,newdata=CD_test)
pred_tree$final <- colnames(pred_test_1)[apply(pred_test_1,1,which.max)]
summary(CD_test$High)
pred_tree$final <- as.factor(pred_tree$final)
CrossTable(CD_test$High,pred_tree$final)
confusionMatrix(CD_test$High,pred_tree$final)
confusionMatrix(CD_test$High,pred_tree$final)
##### Using C5.0 function
intraininglocal<-createDataPartition(CD$High, p = .75, list = F)
training<-CD[intraininglocal,]
testing<-CD[-intraininglocal,]
View(training)
model<-C5.0(training$High~., data = training[,-1])  #Trial - Boosting parameter
plot(model)
#Generate the model summary
summary(model)
#Predict for test data set
pred<- predict.C5.0(model, newdata=testing[,-1])
CrossTable(testing$High,pred)
confusionMatrix(testing$High,pred) #Accuracy is 83.84%
model2<-C5.0(training$High~., data = training[,-1],trials=100)  #Trial - Boosting parameter
plot(model2)
pred2<- predict.C5.0(model2, newdata=testing[,-1])
CrossTable(testing$High,pred2)
confusionMatrix(testing$High,pred2) #Accuracy is 83.64%
plot(Model2,cex=0.5)
confusionMatrix(testing$High,pred) #Accuracy is 83.84%
png(file = "decision_tree.png")
opall_tree = ctree(Risky_Good ~ Undergrad + Marital.Status + City.Population +
Work.Experience + Urban, data = FC)
summary(opall_tree)
plot(opall_tree)
FraudCheck <- read.csv(file.choose())
Risky_Good = ifelse(FraudCheck$Taxable.Income<= 30000, "Risky", "Good")
FC = data.frame(FraudCheck,Risky_Good)
Risky_Good = ifelse(FraudCheck$Taxable.Income<= 30000, "Risky", "Good")
FC = data.frame(FraudCheck,Risky_Good)
barplot(table(FC$Risky_Good))
barplot(table(FC$Risky_Good))
# balancing the data
balance <- as.integer(sample(rownames(FC[Risky_Good=="Good",]),124,replace = F))
train_F <- FC_1[intraininglocal,];
# balancing the data
balance <- as.integer(sample(rownames(FC[Risky_Good=="Good",]),124,replace = F))
Risky_Good = ifelse(FraudCheck$Taxable.Income<= 30000, "Risky", "Good")
FC = data.frame(FraudCheck,Risky_Good)
View(train_F)
pair(FraudCheck)
pairs(FraudCheck)
pairs.default(FraudCheck)
FC = data.frame(FraudCheck,Risky_Good)
library(partykit)
library(C50)
library(tree)
library(gmodels)
library(caret)
library(knitr)
library(png)
FraudCheck <- read.csv(file.choose())
View(FraudCheck)
hist(FraudCheck$Taxable.Income)
Risky_Good = ifelse(FraudCheck$Taxable.Income<= 30000, "Risky", "Good")
FC = data.frame(FraudCheck,Risky_Good)
FC_train <- FC[1:300,]
FC_test <- FC[301:600,]
png(file = "decision_tree.png")
opall_tree = ctree(Risky_Good ~ Undergrad + Marital.Status + City.Population +
Work.Experience + Urban, data = FC)
summary(opall_tree)
plot(opall_tree)
plot(opall_tree)
png(file = "decision_tree.png")
op_tree = ctree(Risky_Good ~ Undergrad + Marital.Status + City.Population +
Work.Experience + Urban, data = FC_train)
plot(op_tree)
pred_tree <- as.data.frame(predict(op_tree,newdata=FC_test))
pred_tree["final"] <- NULL
pred_test_df <- predict(op_tree,newdata=FC_test)
CrossTable(FC_test$Risky_Good,pred_test_df)
confusionMatrix(FC_test$Risky_Good,pred_test_df) # 82 % Accuracy
Tree <- C5.0(train_F$Risky_Good~.,data = train_F,trials=50)
predf <- predict(Tree,test_F)
train_F <- FC_1[intraininglocal,];
intraininglocal <- as.integer(sample(x = nrow(FC_1),size = nrow(FC_1)*.7,F))
length(intraininglocal)
train_F <- FC_1[intraininglocal,];
test_F <- FC_1[-intraininglocal,];
Tree <- C5.0(train_F$Risky_Good~.,data = train_F,trials=50)
predf <- predict(Tree,test_F)
table(Actual =test_F$Risky_Good ,Predicted = predf)
mean(test_F$Risky_Good==predf)
Tree <- C5.0(train_F$Risky_Good~.,data = train_F[,-3],trials=50)
predf <- predict(Tree,test_F)
table(Actual =test_F$Risky_Good ,Predicted = predf)
mean(test_F$Risky_Good==predf)
predf <- predict(Tree,test_F[,-3])
table(Actual =test_F$Risky_Good ,Predicted = predf)
mean(test_F$Risky_Good==predf)
# Data(Train)
train_sal <- read.csv(file.choose())
str(train_sal)
View(train_sal)
View(train_sal)
train_sal$educationno <- as.factor(train_sal$educationno)
class(train_sal)
summary(train_sal)
# Data(Test)
test_sal <- read.csv(file.choose())
str(test_sal)
View(test_sal)
class(test_sal)
library(kernlab)
library(caret)
library(plyr)
library(ggplot2)
library(psych)
library(e1071)
#Visualization
# Plot and ggplot
ggplot(data=train_sal,aes(x=train_sal$Salary, y = train_sal$age, fill = train_sal$Salary)) +
geom_boxplot() +
ggtitle("Box Plot")
plot(train_sal$workclass,train_sal$Salary)
plot(train_sal$education,train_sal$Salary)
plot(train_sal$educationno,train_sal$Salary)
plot(train_sal$maritalstatus,train_sal$Salary)
plot(train_sal$occupation,train_sal$Salary)
plot(train_sal$relationship,train_sal$Salary)
plot(train_sal$race,train_sal$Salary)
plot(train_sal$sex,train_sal$Salary)
ggplot(data=train_sal,aes(x=train_sal$Salary, y = train_sal$capitalgain, fill = train_sal$Salary)) +
geom_boxplot() +
ggtitle("Box Plot")
ggplot(data=train_sal,aes(x=train_sal$Salary, y = train_sal$capitalloss, fill = train_sal$Salary)) +
geom_boxplot() +
ggtitle("Box Plot")
ggplot(data=train_sal,aes(x=train_sal$Salary, y = train_sal$hoursperweek, fill = train_sal$Salary)) +
geom_boxplot() +
ggtitle("Box Plot")
ggplot(data=train_sal,aes(x = train_sal$age, fill = train_sal$Salary)) +
geom_density(alpha = 0.9, color = 'Violet')
ggtitle("Age - Density Plot")
ggplot(data=train_sal,aes(x = train_sal$workclass, fill = train_sal$Salary)) +
geom_density(alpha = 0.9, color = 'Violet')
ggtitle("Workclass Density Plot")
ggplot(data=train_sal,aes(x = train_sal$education, fill = train_sal$Salary)) +
geom_density(alpha = 0.9, color = 'Violet')
ggtitle("education Density Plot")
# Building model
model1<-ksvm(train_sal$Salary~.,
data= train_sal, kernel = "vanilladot")
mean(pred_vanilla==test_sal$Salary) # 93.075
pred_vanilla<-predict(model1,newdata=test_sal)
pred_vanilla<-predict(model1,test_sal)
# Building model
model1<-ksvm(train_sal$Salary~.,
data= train_sal, kernel = "vanilladot")
pred_vanilla<-predict(model1,test_sal)
model1
Salary_prediction <- predict(model1, test_sal)
# Data(Test)
test_sal <- read.csv(file.choose())
test_sal$educationno <- as.factor(test_sal$educationno)
Salary_prediction <- predict(model1, test_sal)
mean(Salary_prediction==test_sal$Salary) # 93.075
# kernel = rfdot
model_rfdot<-ksvm(train_sal$Salary~.,
data= train_sal,kernel = "rbfdot")
pred_rfdot<-predict(model_rfdot,newdata=test_sal)
mean(pred_rfdot==test_sal$Salary) # 85.19
# kernal = besseldot
model_besseldot<-ksvm(train_sal$Salary~..,data = train_sal,kernel = "besseldot")
# kernal = besseldot
model_besseldot<-ksvm(train_sal$Salary~.,data = train_sal,kernel = "besseldot")
pred_bessel<-predict(model_besseldot,newdata=test_sal)
mean(pred_bessel==train_sal$Salary)
model_poly<-ksvm(train_sal$Salary ~.,data = train_sal,kernel = "polydot")
pred_poly<-predict(model_poly,newdata = test_sal)
mean(pred_poly==train_sal$Salary) # 83.925
forestfire <- read.csv(file.choose())
str(forestfire)
summary(forestfire)
View(forestfire)
# The area value has lots of zeros
hist(forestfire$area)
rug(forestfire$area)
# Transform the Area value to Y
forestfire1 <- mutate(forestfire, y = log(area + 1))  # default is to the base e, y is lower case
library(kernlab)
library(caret)
library(plyr)
library(ggplot2)
library(psych)
library(e1071)
# Transform the Area value to Y
forestfire1 <- mutate(forestfire, y = log(area + 1))  # default is to the base e, y is lower case
hist(forestfire1$y)
normalize<-function(x){
return ( (x-min(x))/(max(x)-min(x)))
}
forestfire$temp = normalize(forestfire$temp)
forestfire$RH   = normalize(forestfire$RH)
forestfire$wind = normalize(forestfire$wind)
forestfire$rain = normalize(forestfire$rain)
# We need to tweak this as a classification problem.lets base out the Size using this criteria :
attach(forestfire)
# Data Partition
set.seed(123)
ind <- sample(2, nrow(FF), replace = TRUE, prob = c(0.7,0.3))
ind <- sample(2, nrow(forestfire), replace = TRUE, prob = c(0.7,0.3))
forestfire_train <- forestfire[ind==1,]
forestfire_test  <- forestfire[ind==2,]
# Building model
model1<-ksvm(size_category~temp+rain+wind+RH,
data=forestfire_train,kernel = "vanilladot")
model1
Area_pred <- predict(model1, forestfire_test)
mean(Area_pred==forestfire$size_category)
# kernel = rfdot
model_rfdot<-ksvm(size_category~temp+rain+wind+RH,
data=forestfire_train,kernel = "rfdot")
# kernel = rfdot
model_rbfdot<-ksvm(size_category~temp+rain+wind+RH,
data=forestfire_train,kernel = "rbfdot")
model_rbfdot
Area_pred_rbfdot <- predict(model_rbfdot, forestfire_test)
mean(Area_pred_rbfdot==forestfire$size_category) # 73.11%
# kernal = besseldot
model_besseldot<-ksvm(size_category~temp+rain+wind+RH,
data=forestfire_train,kernel = "besseldot")
model_besseldot
Area_pred_besseldot <- predict(model_besseldot, forestfire_test)
mean(Area_pred_besseldot==forestfire$size_category) # 73.11%
# kernel = polydot
model_polydot<-ksvm(size_category~temp+rain+wind+RH,
data=forestfire_train,kernel = "polydot")
model_polydot
Area_pred_polydot <- predict(model_polydot, forestfire_test)
mean(Area_pred_polydot==forestfire$size_category) # 73.11%
model1<-ksvm(size_category~temp+rain+wind+RH,
data=forestfire_train,kernel = "vanilladot")
model1
mean(Area_pred_polydot==forestfire$size_category) # 73.11%
# kernel = polydot
model_polydot<-ksvm(size_category~temp+rain+wind+RH,
data=forestfire_train,kernel = "polydot")
model_polydot
Area_pred_polydot <- predict(model_polydot, forestfire_test)
mean(Area_pred_polydot==forestfire$size_category) # 73.11%
# kernel = laplacedot
model_laplacedot<-ksvm(size_category~temp+rain+wind+RH,
data=forestfire_train,kernel = "laplacedot")
model_laplacedot
Area_pred_laplacedot <- predict(model_laplacedot, forestfire_test)
mean(Area_pred_laplacedot==forestfire$size_category) # 73.11%
# kernel = matrix
model_matrix<-ksvm(size_category~temp+rain+wind+RH,
data=forestfire_train,kernel = "matrix")
# kernel = matrix
model_anovadot<-ksvm(size_category~temp+rain+wind+RH,
data=forestfire_train,kernel = "anovadot")
model_anovadot
Area_pred_anovadot <- predict(model_anovadot, forestfire_test)
mean(Area_pred_anovadot==forestfire$size_category)
library(rmarkdown)
library(tm)
library(e1071)
library(gmodels)
library(caret)
sms_data<-read.csv(file.choose(),stringsAsFactors = F)
class(sms_data)
View(sms_data)
summary(sms_data)
str(sms_data)
sms_data$type<-as.factor(sms_data$type)
str(sms_data)
table(sms_data$type)
hist(sms_data$type)
a<-table(sms_data$type)
hist(a)
sms_corpous<-VCorpus(VectorSource(sms_data$text))
class(sms_corpous)
# Cleaning data (removing unwanted symbols)
corpus_clean<-tm_map(sms_corpous,tolower)
corpus_clean<-tm_map(corpus_clean, removeNumbers)
corpus_clean<-tm_map(corpus_clean,removeWords, stopwords())
corpus_clean<-tm_map(corpus_clean,removePunctuation)
corpus_clean<-tm_map(corpus_clean,stripWhitespace)
corpus_clean <- tm_map(corpus_clean, PlainTextDocument)
class(corpus_clean)
sms_dtm <- DocumentTermMatrix(corpus_clean)
class(sms_dtm)
# creating training and test datasets
sms_raw_train <- sms_data[1:4169, ]
sms_raw_test  <- sms_data[4170:5559, ]
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test  <- sms_dtm[4170:5559, ]
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test  <- corpus_clean[4170:5559]
# check that the proportion of spam is similar
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
# indicator features for frequent words
# if the word has been referred to 5 times or more
sms_dict<-findFreqTerms(sms_dtm, 5)
# Now apply this particular dictionary of words to training and testing data.
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test  <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
temp <- as.data.frame(as.matrix(sms_train))
View(temp)
dim(sms_train)
dim(sms_test)
#inspect(sms_corpus_train[1:100])
#list(sms_dict[1:100])
# convert counts to a factor
# Create a custom function to show that if a specific word as been used more than once.
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
}
# apply() convert_counts() to columns of train/test data
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_test, MARGIN = 2, convert_counts)
View(sms_train)
View(sms_test)
##  Training a model on the data ----
# Now apply naiveBayes Model on the new sms_train and original data
# on Type (Classifier for ham or spam)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_classifier
##  Evaluating model performance ----
sms_test_pred <- predict(sms_classifier, sms_test)
CrossTable(sms_test_pred,sms_raw_test$type,prop.chisq = FALSE,prop.t = FALSE,
prop.r = FALSE, dnn = c('predicted', 'actual'))
confusionMatrix(sms_test_pred,sms_raw_test$type)
library(rvest)
library(XML)
library(magrittr)
library(tm)
library(wordcloud)
library(wordcloud2)
install.packages("wordcloud2")
library(wordcloud2)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(reshape2)
library(dplyr)
library(scales)
# IMDBReviews #############################
aurl <- "https://www.imdb.com/title/tt0419058/reviews?ref_=tt_ql_3"
IMDB_reviews <- NULL
for (i in 1:10){
murl <- read_html(as.character(paste(aurl,i,sep="=")))
rev <- murl %>%
html_nodes(".show-more__control") %>%
html_text()
IMDB_reviews <- c(IMDB_reviews,rev)
}
length(IMDB_reviews)
setwd("D:/Assignment/Text Mining")
write.table(IMDB_reviews,"PHP.txt",row.names = F)
PHP <- read.delim('PHP.txt')
str(PHP)
View(PHP)
# Build Corpus and DTM/TDM
library(tm)
corpus <- PHP[-1,]
head(corpus)
class(corpus)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# Clean the text
corpus <- tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removePunctuation)
inspect(corpus[1:5])
inspect(corpus[1:5])
corpus_clean<-tm_map(corpus,stripWhitespace)
inspect(corpus[1:5])
cleanset<-tm_map(corpus,removeWords, stopwords('english'))
removeURL <- function(x) gsub('http[[:alnum:]]*','',x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:5])
cleanset<-tm_map(cleanset,removeWords, c('can','film'))
cleanset<-tm_map(cleanset,removeWords, c('movie','movies'))
cleanset <- tm_map(cleanset, gsub,pattern = 'character', replacement = 'characters')
inspect(cleanset[1:5])
cleanset <- tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])
#Term Document Matrix :
# Convert the unstructured data to structured data :
tdm <- TermDocumentMatrix(cleanset)
tdm
# the terms indicate that there are 708 words and 389 documents(# of tweets) in this TDM
# Sparsity is 96% which indicates that there are lots of zero values.
tdm <- as.matrix(tdm)
tdm[1:10,1:20]
# Bar Plot
w <- rowSums(tdm)  # provides the no of times a particular word has been used.
w <- subset(w, w>= 50) # Pull words that were used more than 25 times.
barplot(w, las = 2, col = rainbow(50))
window()
windows()
barplot(w, las = 2, col = rainbow(50))
# Word Cloud :
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
library(wordcloud2)
w <- data.frame(names(w),w)
colnames(w) <- c('word','freq')
wordcloud2(w,size = 0.5, shape = 'triangle', rotateRatio = 0.5,
minSize = 1)
letterCloud(w,word = 'A',frequency(5), size=1)
# Sentiment Analysis for tweets:
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
# Read File
IMDB_reviews <- read.delim('PHP.TXT')
reviews <- as.character(IMDB_reviews[-1,])
class(reviews)
# Obtain Sentiment scores
s <- get_nrc_sentiment(reviews)
head(s)
reviews[4]
get_nrc_sentiment('splendid')
# Splendid has one Joy and one positive
get_nrc_sentiment('no words') #1 Anger and 1 Negative
barplot(colSums(s), las = 2.5, col = rainbow(10),
ylab = 'Count',main= 'Sentiment scores for IMDB Reviews
for Phir Hera Pheri')
